---
title: "Cross Validation"
author: "Mounika Chevva (Advisor: Dr. Samantha Seals)"
date: " 09/08/2024"
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


## Summary

## Note 1 [ @domingo2022cross ]

The paper "Cross Validation Voting for Improving CNN Classification in Grocery Products" presents a method to enhance the performance of convolutional neural networks (CNNs) in classifying grocery products. Without implementing numerous classifier types, the authors present Cross-Validation-Voting (CVV), a method that takes the benefit of ensemble learning and cross-validation. To ensure that every model is optimized for a distinct validation set, CVV leverages multiple instances of the same classifier to train on different subsets of data. By doing so, the models' capacity for generalization is enhanced and overfitting is lessened.

The efficacy of CVV is demonstrated in the study through trials on a popular grocery stores product dataset, where CVV surpasses the state-of-the-art techniques in terms of F1-score, accuracy, and precision. By training models with fewer epochs and not appreciably extending the training duration, the CVV technique produces better outcomes. Additionally, the method is flexible to other CNN concepts and can be modified to apply to other classifier types, such as SVM and boosting approaches.

## Note 2 [ @yates2023cross ]

The article "Cross Validation for Model Selection: A Review with Examples from Ecology" by Luke A. Yates et al. provides a comprehensive review of cross-validation techniques for model selection in ecological research. 
The authors highlight the importance of cross-validation, a data-splitting method, for assessing and contrasting statistical models, particularly in situations when it is difficult to determine the model's likelihood or when parameter counting is difficult. Technical details that are frequently missed in the ecological literature are brought to light in this review, including estimating uncertainty, risk of overfitting, and bias correction.

When there are fewer than ten folds (k), the authors advise utilizing k-fold cross-validation with bias correction or leave-one-out cross-validation (LOO-CV) in order to reduce bias. They also advocate for the selection of the simplest model that performs similarly to the best-scoring model, and they present a modified one-standard-error criteria to mitigate overfitting. With the help of two ecological case studies, the paper illustrates how to apply these methodologies and offers helpful advice. The paper concludes by highlighting the significance of predictive evaluation in model selection for a range of modeling objectives, such as hypothesis testing, exploration, and prediction, and by warning against forming biased conclusions after model selection.

## Note 3 [ @qi2019estimating ]

The paper "On Estimating Model in Feature Selection With Cross-Validation" investigates the effectiveness of different cross-validation (CV) methods for model evaluation during feature selection. The study examines five CV methods: leave-one-out (LOO), repeated 2-fold CV, repeated 5-fold CV, repeated 10-fold CV, and a nested 10-fold CV. It focuses on the hybrid feature selection algorithm FDHSFFS. The study tests the accuracy of error estimation, computational efficiency, and the choice of best models and feature subsets using four UCI datasets with different feature dimensions and sample subsets.

It includes the lowest errors produced with the least computational expense on low-dimensional datasets by 2-fold CV and LOO. Nested CV and 10-fold CV are computationally demanding yet yield more accurate error estimates on high-dimensional datasets. Though different CV techniques may use different optimal models, they frequently provide roughly the same optimal feature subsets. The nested 10-fold CV approach strikes a balance between computing cost and accuracy by achieving almost unbiased error estimation.

## Note 4 [ @6322811 ]

The paper "Design and Realization of the Parallel Computing Framework of Cross-Validation" introduces Parallel-CV, a multi-threaded framework designed to enhance the efficiency of cross-validation (CV) in machine learning. 
Due to their reliance on serial execution, traditional CV approaches are resource-intensive and slow, especially when dealing with large datasets. In order to overcome this, Parallel-CV speeds up the procedure by dividing the workload among several CPU cores.

With the help of the framework's straightforward API and support for a number of CV techniques, like K-Fold and Leave-M-Out, users may parallelize machine learning jobs without needing to know anything about parallel programming. Parallel-CV showed notable calculation time improvements in studies with different sized datasets, particularly for larger datasets. Although efficiency marginally declines with more threads, the framework nonetheless retains scalability and assures load balancing. In general, Parallel-CV streamlines the parallelization process while providing excellent scalability, adaptability, and efficiency for cross-validation workloads.

## Note 5 [ @6098566 ]

The paper "Determining the Repeat Number of Cross-Validation" proposes two methods, FCI (Fixed Confidence Interval) and TSE (Two-Step Estimation), to determine the optimal number of repetitions in cross-validation (CV). 

A popular method for estimating classification error rates is cross-validation, especially when dealing with gene expression data. The amount of CV repetitions is typically determined empirically, which could produce inaccurate or ineffective findings. The study focuses attention to the limitations of this empirical strategy and offers more dependable techniques based on approximative confidence intervals.

The number of repetitions is automatically adjusted by the FCI and TSE algorithms based on the data, folds, and classification model. By putting these techniques to the test on actual datasets, the authors discovered that they can estimate error rates with the appropriate accuracy while cutting down on pointless computations. Comparing these approaches to the fixed empirical methodology, they offer a more accurate and dynamic way to determine repeat counts. In order to increase the accuracy of error rate estimates in CV, the study highlights the necessity of adaptive approaches.

## Note 6 [ @9655795 ]

 The paper "Time-Series Cross-Validation Parallel Programming using MPI" focuses on optimizing time-series cross-validation (TSCV) through parallel processing using MPI (Message Passing Interface).
Time-series data is sequential in nature, which makes parallelization difficult. However, by dividing up the work among several processors, this research seeks to shorten computing times.

In order to divide a dataset into training and testing sets, the article suggests a parallelized TSCV algorithm that grows the size of the training set with each run. Each processor is given a distinct workload by the algorithm, and each one builds and tests models on its own. The performance is assessed in terms of computational time, efficiency, and speed.

The compute time was notably lowered by the parallelized approach, going from 28.33 seconds when using five processors to 15.47 seconds while using five processors, according to the results. However, because of workload imbalance, efficiency dropped with more processors and speedup was non-linear, peaking at 1.8 with five processors. According to its findings, parallel TSCV can save calculation time by 42% for large datasets. It draws attention to the need for more load balancing optimization and makes suggestions for looking at other parallel frameworks, such as OpenMP.








## References
