---
title: "Prediction of Martensite Start Temperature in Steels Using Cross Validation"
author: "Mounika Chevva (Advisor: Dr. Samantha Seals)"
date: 'today'
format:
  html:
    code-fold: true
course: IDC6490 Capstone in Data Science
bibliography: references.bib 
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


[Slides](slides.html)

## Introduction

Cross-validation is a fundamental statistical technique widely used in machine learning for assessing the performance and generalizability of predictive models. It involves partitioning a dataset into training and validation subsets, allowing a model to be trained on one subset while being evaluated on another. 

One of the most common forms of cross-validation is K-Fold Cross-Validation, introduced by Kohavi (1995)@Kohavi1995. In this method, the dataset is divided into $k$ subsets (or "folds"). The model is trained on $k‚àí1$ folds and validated on the remaining fold, a process that is repeated 
$k$ times, ensuring each fold serves as a validation set exactly once. This technique provides a more reliable estimate of model performance by averaging the results over multiple training and validation cycles.

## Methods

Model Measures of Errors are fundamental for evaluating the performance of predictive models, especially in regression tasks. These metrics help assess how well the model approximates the true relationship between the predictor and target variables. Below are some common measures of error, along with their mathematical formulas and references:


### Root Mean Squared Error (RMSE)

RMSE, or Root Mean Squared Error, tells us how far off our model‚Äôs predictions are from the actual values, on average. It looks at the squared differences between predicted and real values, which means it gives more weight to bigger errors, making it especially useful for spotting when predictions miss the mark by a lot. This makes RMSE great for identifying how well a model handles extreme cases, where predictions are far off from what's expected (Chai & Draxler, 2014)@chai2014root .

$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
where $y_i$ is the observed value, $\hat{y}_i$ is the predicted value,and $n$ is the total number of observations.

### Mean Absolute Error (MAE)

Mean Absolute Error (MAE) calculates the average of the absolute differences between the observed and predicted values, making it a straightforward way to understand model accuracy. Unlike RMSE, MAE doesn‚Äôt penalize large errors as much, so it‚Äôs less sensitive to outliers. This makes it useful when you want an overall idea of prediction accuracy, expressed in the same units as the target variable (Willmott & Matsuura, 2005)@willmott2005advantages.

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$


###  R-squared (R¬≤)

R¬≤, or the coefficient of determination, indicates how much of the variance in the dependent variable can be explained by the independent variables in a model. While it's a popular metric for assessing how well a model fits the data, Draper and Smith (1998) @draper1998applied warn that R¬≤ can sometimes be misleading, particularly in the context of non-linear models. Therefore, while R¬≤ can provide valuable insights, it should be interpreted with caution and in conjunction with other performance metrics to ensure a comprehensive evaluation of model performance.
$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$
where $\bar{y}$ is the mean of the actual values.

##  k-Fold Cross-validation

K-Fold Cross-Validation @arlot2010survey divides the dataset into $ùëò$ equal parts, or "folds." The model trains on $k‚àí1$ folds and tests on the remaining one, repeating this for each fold. This approach helps prevent overfitting and provides a clearer picture of how well the model performs on unseen data

### Step-by-Step Procedure for K-Fold Cross-Validation

Step 1: Prepare the Dataset
Begin by loading the dataset and selecting relevant predictor variables along with the target variable. For instance, if predicting Martensite Start Temperature (Ms), ensure that the predictors (C, Mn, Si, Cr, Ni) are included.

Step 2: Set Up K-Fold Control
Define the number of folds (commonly 5 or 10) for the cross-validation. Set a random seed for reproducibility and create a training control object using the trainControl function from the caret package.

Step 3: Fit the Model
Train a linear regression model with K-Fold Cross-Validation by specifying the model formula and the dataset, while using the training control object that defined earlier.

Step 4: Evaluate Performance
Once the model is trained, review the performance metrics, such as RMSE and R¬≤, provided by the train function.

Step 5: Analyze Results
Extract and visualize the results to understand the model's stability across different folds, ensuring a comprehensive evaluation of its predictive capabilities. 

## Leave-one-out Cross-validations (LOOCV)

Leave-One-Out Cross-Validation (LOOCV) @geisser1975predictive is a specific case of cross-validation where each training set is created by leaving out just one observation from the dataset. This technique is useful for small datasets, as it maximizes the training data available for each model training.

#### Step-by-Step Procedure for LOOCV:

1.Prepare the Dataset: Considering  the Martensite dataset, which consists of observations with various chemical compositions (C, Mn, Si, Cr, Ni) and the target variable, Martensite Start Temperature (Ms).

2.Set Up for LOOCV: With $n$observations in the dataset, LOOCV will create $n$ folds, where each fold contains one observation as the test set.

3.Iterate Through Each Observation: For each observation:

Leave Out One Observation: Select one data point (e.g., a specific steel composition with its Ms value) as the test set.

Train the Model: Use the remaining $n‚àí1$ observations to train a linear regression model predicting Ms based on the chemical compositions.

4.Evaluate the Model: After training, predict the Ms value for the left-out observation and record the performance metric, such as RMSE or MAE.

5.Repeat for All Observations: Continue this process until every observation has served as the test set.

6.Calculate Average Performance: Finally, compute the average of the recorded metrics to assess the model's overall predictive performance on the Martensite dataset.

## Nested Cross-validation

Nested Cross-Validation is a robust model evaluation technique that involves two levels of cross-validation: an outer loop for assessing model performance and an inner loop for hyperparameter tuning @boulesteix2007partial. This approach is particularly useful for avoiding overfitting and ensuring that the model generalizes well to unseen data.

#### Step-by-Step Procedure for Nested Cross-Validation:

1.Dataset Preparation: Start with the Martensite dataset containing steel compositions and Martensite Start Temperature (Ms).

2.Outer Loop: Split the dataset into $k$ folds (e.g., 5-fold) for performance evaluation.

3.Inner Loop: For each outer fold, perform $m$ folds (e.g., 5-fold) for hyperparameter tuning.

4.Train Model: Train the model on $m‚àí1$ folds and validate on the held-out fold in the inner loop.

5.Select Hyperparameters: Choose the best hyperparameters based on inner loop performance.

6.Evaluate Model: Train the final model on the outer training set and test it on the held-out fold.

7.Average Metrics: Calculate average performance metrics across all outer folds to assess model effectiveness.

## Data Extraction, Transformation and Visualization

In our study, we analyzed a dataset from @wentzien2024machine Martensite dataset focuses on predicting the Martensite Start Temperature (Ms) in steel alloys based on their chemical compositions. It includes various attributes such as carbon (C), manganese (Mn), silicon (Si), chromium (Cr), and nickel (Ni) concentrations, which are known to influence the transformation of austenite to martensite during cooling  [Table 1](#table-1).

#### Load the Data and Libraries

```{r}
library(readr)

Martensite <- read.csv("C:/Users/Mounika/Documents/IDC6940_CrossValidation/Martensite Start-Temp.csv")
#head(Martensite)
#str(Martensite)


```

####  Dataset Description

The martensite start temperature (Ms) is a critical parameter when designing high-performance
steels and their heat treatments.

Martensite start temperature (Ms) is our target variable.

This table categorizes the variables based on their type and description to our study.

#### Table 1

```{r table-1, echo=FALSE}

library(knitr)

var_desc <- data.frame(
  Var = c("C", "Mn", "Si", "Cr", "Ni", "Mo", "V", "Co", "Al", 
               "W", "Cu", "Nb", "Ti", "B", "N", "Ms"),
  Description = c(
    "Carbon ",
    "Manganese ",
    "Silicon ",
    "Chromium ",
    "Nickel ",
    "Molybdenum ",
    "Vanadium",
    "Cobalt ",
    "Aluminum",
    "Tungsten ",
    "Copper ",
    "Niobium ",
    "Titanium ",
    "Boron ",
    "Nitrogen ",
    "Martensite Start Temperature Target Variable(¬∞C)"
  ),
  Type = c(
    "Numeric","Numeric","Numeric", "Numeric", "Numeric", "Numeric", "Numeric", "Numeric",
   "Numeric", "Numeric", "Numeric", "Numeric", "Numeric", "Numeric", "Numeric", "Numeric"
  ),
  stringsAsFactors = FALSE  
)

kable(var_desc)


```


#### Exploratory Data Analysis (EDA)

Exploratory Data Analysis (EDA) on the Martensite Start Temperature (Ms) dataset involves understanding the distribution, relationships, and patterns in the data.

```{r}
#summary(Martensite)
sum(is.na(Martensite))

```

The output [1] 0 from sum(is.na(Martensite)) indicates that there are no missing values (NAs) in the Martensite dataset. This means that all entries in the dataset have complete data without any gaps.

### Data Visualization

#### Histogram for Martensite Start Temperature 

```{r Ms-distribution, fig.cap="Figure 1: Distribution of Martensite Start Temperature (Ms)"}

library(ggplot2)
Ms_distribution <- ggplot(Martensite, aes(x = Ms)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Martensite Start Temperature (Ms)", x = "Martensite Start Temperature", y = "Frequency")

Ms_distribution

ggsave(filename = "Ms_distribution.png", plot = Ms_distribution, width = 8, height = 6, dpi = 300)



```
The histogram describes  the distribution of Martensite Start Temperature (Ms) values[Figure 1: Distribution of Martensite Start Temperature (Ms)](Ms_distribution.png), ranging from 200 to 800¬∞C. It shows a bimodal distribution with peaks around 400-450¬∞C and 600-650¬∞C, indicating two predominant temperature ranges. 

#### Correlation matrix 

The correlation matrix is a powerful tool used to visualize and quantify the relationships between key variables[Figure 2: Correlation_Matrix](correlation_matrix_plot.png)

```{r correlation-matrix, fig.cap="Figure 2: Correlation_Matrix"}
library(ggplot2)
library(reshape2)
library(corrplot)

correlation_matrix <- cor(Martensite, use = "complete.obs")

#print(correlation_matrix)

corr_plot <- corrplot(correlation_matrix, method = "circle")
png("correlation_matrix_plot.png", width = 900, height = 1000)
corrplot(correlation_matrix, method = "circle")
dev.off()

```

#### Insights from correlation matrix 

The matrix uses color intensity to indicate the strength of the relationships between variables.

Correlation coefficients displayed in the matrix range from -1 to 1, where values near ¬±1 indicate strong positive or negative correlations, and values near 0 indicate weak or no linear relationship.

The correlations help identify significant predictors of Martensite:

C (Carbon) shows a strong positive correlation with Mn (Manganese), suggesting that as carbon content increases, manganese also tends to increase.

Ms (Martensite Start Temperature) has several notable negative correlations with elements like V (Vanadium), W (Tungsten), and N (Nitrogen). This implies that higher concentrations of these elements may lead to lower martensite start temperatures.

The relationships identified here can be utilized to build predictive models for Ms based on the chemical composition of alloys.

#### Boxplot for Ms vs Carbon (C)

This boxplot graph illustrates the relationship between the Martensite Start Temperature (Ms) and Carbon content in percentage (%) [Figure 3: Boxplot of Martensite Ms vs Carbon](Ms_vs_Carbon_boxplot.png)

```{r Ms-vs-C, fig.cap="Figure 3: Boxplot of Martensite Ms vs Carbon"}

Ms_vs_C <- ggplot(Martensite, aes(x = as.factor(C), y = Ms)) +
  geom_boxplot(fill = "red") +
  labs(title = "Boxplot of Martensite Ms vs Carbon", x = "Carbon (%)", y = "Martensite Start Temperature (Ms)")

Ms_vs_C

ggsave(filename = "Ms_vs_Carbon_boxplot.png", plot = Ms_vs_C, width = 8, height = 6, dpi = 300)


```

#### Insights from the Boxplot

X-axis (Carbon %): This represents the percentage of carbon content in the material. The values are spread out across a wide range, indicating different carbon concentrations.

Y-axis (Martensite Start Temperature, Ms): This represents the temperature at which martensite begins to form during the cooling process. The temperature is measured in degrees Celsius.

As the carbon content increases, the Ms temperature decreases. This is typical, as higher carbon content usually lowers the Ms temperature in steels and alloys.

## Modelling and Results

### Linear Regression Model

Linear regression is a fundamental statistical technique that establishes a relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. In our dataset, which focuses on predicting the Martensite Start Temperature (Ms) of steel based on its chemical composition (C, Mn, Si, Cr, Ni), linear regression allows us to quantify how changes in these elements influence Ms.

#### Preparing and Training a Linear Regression Model on the Martensite Dataset

```{r}

library(caret)
library(dplyr)
library(knitr)


predictors <- Martensite[, c("C","Mn","Si","Cr","Ni")]
target <- Martensite$Ms 
data_combined <- data.frame(predictors, Ms = target)


set.seed(123)
trainIndex <- createDataPartition(data_combined$Ms, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

model_train <- train(Ms ~ C + Mn + Si + Cr + Ni, 
               data = train_data, 
               method = "lm")

model_summary <- summary(model_train$finalModel)

coefficients_table <- data.frame(
  Term = rownames(model_summary$coefficients),
  Estimate = model_summary$coefficients[, "Estimate"],
  Std_Error = model_summary$coefficients[, "Std. Error"],
  t_value = model_summary$coefficients[, "t value"],
  p_value = model_summary$coefficients[, "Pr(>|t|)"]
)
rownames(coefficients_table) <- NULL

kable(coefficients_table, caption = "Linear Regression Model Coefficients")

```
 $$ M_s = \beta_0 +\beta_1 C +\beta_2 Mn + \beta_3 Si + \beta_4 Cr + \beta_5 Ni $$
$$M_s = 746.99 - 254.85 C - 24.24 Mn - 13.28 Si - 7.8 Cr - 14.64 Ni $$
This model indicates a strong relationship between the chemical compositions and the Martensite Start Temperature, with a high R-squared value signifying that approximately 74.33% of the variability in $Ms$ can be explained by the model.
  

####  k-fold cross-validation & calculation of RMSE, MAE, and R-squared


```{r}
library(caret)

train_kfold <- trainControl(method = "cv", number = 5)

model_kfold <- train(Ms ~ C + Mn + Si + Cr + Ni, 
                     data = train_data, 
                     method = "lm", 
                     trControl = train_kfold)

calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

calculate_r2 <- function(actual, predicted) {
  1 - sum((predicted - actual)^2) / sum((actual - mean(actual))^2)
}

kfold_predictions <- predict(model_kfold, newdata = test_data) 

kfold_rmse <- calculate_rmse(test_data$Ms, kfold_predictions) %>% round(2)
kfold_mae <- calculate_mae(test_data$Ms, kfold_predictions) %>% round(2)
kfold_r2 <- calculate_r2(test_data$Ms, kfold_predictions) %>% round(2)

Measure_of_Error <- c("RMSE", "MAE", "R2")
Result_Value <- c(kfold_rmse, kfold_mae, kfold_r2)
kfold_df <- data.frame(Measure_of_Error, Result_Value) 

kfold_df %>% kable()



```

#### Implementing Leave-One-Out Cross-Validation (LOOCV) for Linear Regression Model & calculation of RMSE, MAE, and R-squared
```{r}
set.seed(123)
train_control_loocv <- trainControl(method = "LOOCV")

model_loocv <- train(Ms ~ C + Mn + Si + Cr + Ni, 
                     data = train_data, 
                     method = "lm", 
                     trControl = train_control_loocv)

loocv_predictions <- predict(model_loocv, newdata = test_data)

loocv_rmse <- calculate_rmse(test_data$Ms, loocv_predictions) %>% round(2)
loocv_mae <- calculate_mae(test_data$Ms, loocv_predictions) %>% round(2)
loocv_r2 <- calculate_r2(test_data$Ms, loocv_predictions) %>% round(2)

Measure_of_Error <- c("RMSE", "MAE", "R2")
Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
loocv_df <- data.frame(Measure_of_Error, Result_Value) 

loocv_df %>% kable()

```

#### Conducting Nested Cross-Validation for the Linear Regression Model

```{r}
set.seed(123)
outer_train_control <- trainControl(method = "cv", number = 5)
inner_train_control <- trainControl(method = "cv", number = 5)

nested_model <- function(data, indices) {
  train_data <- data[indices,]
  test_data <- data[-indices,]
  
  model <- train(Ms ~ C + Mn + Si + Cr + Ni, 
                 data = train_data, 
                 method = "lm", 
                 trControl = inner_train_control)
  
  predictions <- predict(model, newdata = test_data)
  
  rmse <- calculate_rmse(test_data$Ms, predictions)
  mae <- calculate_mae(test_data$Ms, predictions)
  r2 <- calculate_r2(test_data$Ms, predictions)
  
  return(c(rmse, mae, r2))
}


nested_cv_indices <- createFolds(data_combined$Ms, k = 5, list = TRUE, returnTrain = TRUE)


nested_cv_results <- t(sapply(nested_cv_indices, nested_model, data = data_combined))

nested_cv_rmse <- mean(nested_cv_results[, 1]) %>% round(2)
nested_cv_mae <- mean(nested_cv_results[, 2]) %>% round(2)
nested_cv_r2 <- mean(nested_cv_results[, 3]) %>% round(2)

Measure_of_Error <- c("RMSE", "MAE", "R2")
Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
nested_cv_df <- data.frame(Measure_of_Error, Result_Value)

nested_cv_df %>% kable()

```

#### Comparative Analysis of Cross-Validation Results for Linear Regression Models


```{r}
library(tidyr)


kfold_results  <- kfold_df %>% mutate(Method = "5-Fold") 
LOOCV_results  <- loocv_df %>% mutate(Method = "LOOCV")
nested_results <- nested_cv_df %>% mutate(Method = "Nested CV") 


result_long_df <- combine(kfold_results, LOOCV_results, nested_results)

result_wide_df <- result_long_df %>% 
  pivot_wider(names_from = Method, values_from = Result_Value) #%>% 
  
result_wide_df %>% kable()

```

```{r Results, fig.cap="Figure 4: Cross-Validation Results: Linear Model"}
library(ggplot2)
library(dplyr)

result_long_df <- result_long_df %>%
  mutate(Method = factor(Method, levels = c("5-Fold", "LOOCV", "Nested CV")))

line_plot <- result_long_df %>%
  ggplot(aes(x = Method, y = Result_Value, group = Measure_of_Error, color = Measure_of_Error)) +
  geom_line(size = 1.2) + 
  geom_point(size = 3) +  
  labs(title = "Cross-Validation Results: Linear Model",
       x = "Cross-Validation Method",
       y = "Mean Error") +
  theme_minimal() +
  theme(axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold"),
        title = element_text(face = "bold"),
        legend.title = element_blank())

print(line_plot)

ggsave(filename = "cross_validation_results_line_plot.png", plot = line_plot, width = 8, height = 6, dpi = 300)


```
## Model Performance Analysis

The results of this analysis [Figure 4: Cross-Validation Results: Linear Model](cross_validation_results_line_plot.png) reveal that the models tested with 5-Fold Cross-Validation (5-Fold CV) and Leave-One-Out Cross-Validation (LOOCV) demonstrate impressive predictive accuracy. These two models consistently outshine the Nested CV model, indicating that they are more dependable for making predictions from the dataset.


##  Support Vector Machines (SVM) for Regression (SVR)

Support Vector Machines (SVM) are powerful tools for tasks like classification and regression. They work by finding the best boundary to separate data into categories or fit data points for predictions. By using kernels, SVM can handle complex patterns, making them effective for challenging datasets.

```{r}

library(e1071)
library(caret)

set.seed(123) 


predictors <- Martensite[, c("C","Mn","Si","Cr","Ni")]
target <- Martensite$Ms 
data_combined <- data.frame(predictors, Ms = target)


trainIndex <- createDataPartition(data_combined$Ms, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

svr_model <- train(Ms ~ C + Mn + Si + Cr + Ni, 
                   data = train_data, 
                   method = "svmRadial")  


print(svr_model)
```

The SVM model seems to perform really well with $C=1.00$. This is shown by the fact that it has the lowest RMSE, the highest R-squared, and the lowest MAE when compared to the other values tested. Essentially, this means the model is doing a great job of capturing the key patterns in the data, making it a solid choice for this analysis.


### Support Vector Machines (SVM) using K fold Cross Validation

```{r}
train_kfold <- trainControl(method = "cv", number = 5)

svr_model <- train(Ms ~ C + Mn + Si + Cr + Ni, 
                   data = train_data, 
                   method = "svmRadial",
                   trControl = train_kfold)

svr_kfold_predictions <- predict(svr_model, newdata = test_data)

calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

calculate_r2 <- function(actual, predicted) {
  cor(actual, predicted)^2
}

svr_kfold_rmse <- calculate_rmse(test_data$Ms, svr_kfold_predictions) %>% round(2)
svr_kfold_mae <- calculate_mae(test_data$Ms, svr_kfold_predictions) %>% round(2)
svr_kfold_r2 <- calculate_r2(test_data$Ms, svr_kfold_predictions) %>% round(2)

Measure_of_Error <- c("RMSE", "MAE", "R2")
Result_Value <- c(svr_kfold_rmse, svr_kfold_mae, svr_kfold_r2)
svm_kfold_df <- data.frame(Measure_of_Error, Result_Value) 

svm_kfold_df %>% kable()


```

### Support Vector Machines (SVM) using LOOCV 


```{r}
library(e1071)
library(caret)
library(dplyr)
library(knitr)

set.seed(123)

train_loocv <- trainControl(method = "LOOCV")

model_loocv <- train(Ms ~ C + Mn + Si + Cr + Ni, 
                     data = train_data, 
                     method = "svmLinear",  
                     trControl = train_loocv)

loocv_predictions <- predict(model_loocv, newdata = test_data)

loocv_rmse <- sqrt(mean((test_data$Ms - loocv_predictions)^2)) %>% round(2) 
loocv_mae <- mean(abs(test_data$Ms - loocv_predictions)) %>% round(2) 
loocv_r2 <- cor(test_data$Ms, loocv_predictions)^2 %>% round(2) 

Measure_of_Error <- c("RMSE", "MAE", "R2")
Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
svm_loocv_df <- data.frame(Measure_of_Error, Result_Value) 

kable(svm_loocv_df)


```

### Support Vector Machines (SVM) using Nested Cross Validation

```{r}
set.seed(123)

outer_train_control <- trainControl(method = "cv", number = 5)

inner_train_control <- trainControl(method = "cv", number = 5)

nested_svm <- function(data, indices) {
  train_data <- data[indices,]
  test_data <- data[-indices,]


 model <- train(Ms ~ C + Mn + Si + Cr + Ni, 
                 data = train_data, 
                 method = "svmRadial", 
                 trControl = inner_train_control,
                 tuneLength = 3) 
   predictions <- predict(model, newdata = test_data)
rmse <- sqrt(mean((test_data$Ms - predictions)^2))
  mae <- mean(abs(test_data$Ms - predictions))
  r2 <- cor(test_data$Ms, predictions)^2
  
  return(c(rmse, mae, r2))
}

nested_cv_indices <- createFolds(data_combined$Ms, k = 5, list = TRUE, returnTrain = TRUE)

nested_cv_results <- t(sapply(nested_cv_indices, nested_svm, data = data_combined))

nested_cv_rmse <- mean(nested_cv_results[, 1]) %>% round(2)
nested_cv_mae <- mean(nested_cv_results[, 2]) %>% round(2)
nested_cv_r2 <- mean(nested_cv_results[, 3]) %>% round(2)

Measure_of_Error <- c("RMSE", "MAE", "R2")
Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
svm_nested_cv_df <- data.frame(Measure_of_Error, Result_Value)

kable(svm_nested_cv_df)


```

#### Comparative Analysis of Cross-Validation Results for Support Vector Machines (SVM) Regression Models

```{r}
library(tidyr)


svm_kfold_results  <- svm_kfold_df %>% mutate(Method = "5-Fold") 
svm_LOOCV_results  <- svm_loocv_df %>% mutate(Method = "LOOCV")
svm_nested_results <- svm_nested_cv_df %>% mutate(Method = "Nested CV") 


svm_result_long_df <- combine(svm_kfold_results, svm_LOOCV_results, svm_nested_results)

result_wide_df <- svm_result_long_df %>% 
  
  pivot_wider(names_from = Method, values_from = Result_Value) #%>% 
  
result_wide_df %>% kable()

```

### Model Analysis of SVM Regression

The model was evaluated using 5-Fold Cross-Validation, Leave-One-Out Cross-Validation (LOOCV), and Nested Cross-Validation. The 5-Fold Cross-Validation performed the best, showing the lowest RMSE (35.93), MAE (20.98), and highest R¬≤ (0.90), indicating strong accuracy. LOOCV had higher errors, with RMSE of 52.61, MAE of 28.49, and a lower R¬≤ of 0.79, suggesting less effectiveness. Nested CV provided balanced results, with RMSE of 40.09, MAE of 22.24, and R¬≤ of 0.86.



## Model Comparision Results Table for Linear Regression & SVM Regression

```{r }

library(tidyr)

Comparison_df <- rbind(
  Regression_R <- cbind(result_long_df, c(replicate(length(result_long_df),"Linear_Regression"))) %>% rename(Model=4),
  
  

SVM_KFold_R <- svm_kfold_df %>% mutate(Method = "5-Fold", Model = "SVM"),
 SVM_LOOCV_R <- svm_loocv_df %>% mutate(Method = "LOOCV", Model = "SVM"),
 SVM_Nest_R  <- svm_nested_cv_df %>% mutate(Method = "Nested CV", Model = "SVM")
  ) 

Comparison_df %>% 
  pivot_wider(names_from = Model, values_from = Result_Value) %>% 
  relocate(Method) %>% 
  kable()

```

### Linear Regression & SVM Regression Model Comparison Plot

```{r Model-Comparison, fig.cap="Figure 5: Model Comparison: Linear Regression vs SVM (K-Fold CV, k=5)"}

library(ggplot2)
library(dplyr)

kfold_linear_df <- kfold_df %>% mutate(Model = "Linear Regression")
kfold_svm_df <- svm_kfold_df %>% mutate(Model = "SVM")

comparison_df <- bind_rows(kfold_linear_df, kfold_svm_df)

comparison_plot <- ggplot(comparison_df, aes(x = Measure_of_Error, y = Result_Value, color = Model, group = Model)) +
  geom_line(size = 1.2) +                   
  geom_point(size = 3) +                  
  labs(title = "Model Comparison: Linear Regression vs SVM (K-Fold CV, k=5)",
       y = "Performance Metric Value",
       x = "Performance Metric") +
  theme_minimal() +
  theme(legend.title = element_blank())    

print(comparison_plot)

ggsave(filename = "model_comparison_plot.png", plot = comparison_plot, width = 8, height = 6, dpi = 400)

```
For the martensite start temperature (Ms) dataset,the performance of Linear Regression and Support Vector Machine (SVM) models using three key metrics is compared [Figure 5: Model Comparison: Linear Regression vs SVM (K-Fold CV, k=5)](model_comparison_plot.png) : Mean Absolute Error (MAE), R-squared (R¬≤), and Root Mean Squared Error (RMSE), evaluated with 5-fold cross-validation. The findings clearly show that the SVM model performs better than Linear Regression in all aspects. SVM achieved a notably lower MAE (~21) compared to Linear Regression (~33), meaning its predictions were closer to the actual values. The R¬≤ for SVM (~0.9) was much higher than Linear Regression (~0.55), indicating SVM explained 90% of the data's variance, whereas Linear Regression only captured about 55%. Additionally, SVM's RMSE (~25) was significantly lower than Linear Regression‚Äôs (~48), reflecting its greater accuracy and fewer large prediction errors. Overall, SVM proved to be the more suitable model for predicting martensite start temperature, delivering more accurate and reliable results for this dataset.

## Conclusion

In this study, I compared how well Linear Regression and Support Vector Machine (SVM) models predict the Martensite start temperature (Ms) using 5-fold cross-validation. Our results clearly show that SVM outperformed Linear Regression in every key metric: Mean Absolute Error (MAE), R-squared (R¬≤), and Root Mean Squared Error (RMSE).

SVM achieved a much lower MAE, meaning its predictions were closer to the actual values. It also delivered an R¬≤ of around 0.9, explaining 90% of the data's variability, while Linear Regression only explained about 55%. Furthermore, SVM's RMSE was significantly lower, indicating fewer large prediction errors and better overall accuracy.

These findings highlight that SVM is a more effective and reliable method for predicting martensite start temperature. Its superior performance suggests it‚Äôs a powerful tool for understanding and predicting material properties in materials science. Moving forward, future work could focus on optimizing the SVM model through hyperparameter tuning and feature selection, further boosting its predictive accuracy.








