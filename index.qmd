---
title: "Cross Validation"
#subtitle: "This is a Report Template"
author: " Mounika Chevva (Advisor: Dr. Seals)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


## Summary

## Note 1 [ @domingo2022cross ]

The paper "Cross Validation Voting for Improving CNN Classification in Grocery Products" presents a method to enhance the performance of convolutional neural networks (CNNs) in classifying grocery products. Without implementing numerous classifier types, the authors present Cross-Validation-Voting (CVV), a method that takes the benefit of ensemble learning and cross-validation. To ensure that every model is optimized for a distinct validation set, CVV leverages multiple instances of the same classifier to train on different subsets of data. By doing so, the models' capacity for generalization is enhanced and overfitting is lessened.

The efficacy of CVV is demonstrated in the study through trials on a popular grocery stores product dataset, where CVV surpasses the state-of-the-art techniques in terms of F1-score, accuracy, and precision. By training models with fewer epochs and not appreciably extending the training duration, the CVV technique produces better outcomes. Additionally, the method is flexible to other CNN concepts and can be modified to apply to other classifier types, such as SVM and boosting approaches.

## Note 2 [ @yates2023cross ]

The article "Cross Validation for Model Selection: A Review with Examples from Ecology" by Luke A. Yates et al. provides a comprehensive review of cross-validation techniques for model selection in ecological research. 
The authors highlight the importance of cross-validation, a data-splitting method, for assessing and contrasting statistical models, particularly in situations when it is difficult to determine the model's likelihood or when parameter counting is difficult. Technical details that are frequently missed in the ecological literature are brought to light in this review, including estimating uncertainty, risk of overfitting, and bias correction.

When there are fewer than ten folds (k), the authors advise utilizing k-fold cross-validation with bias correction or leave-one-out cross-validation (LOO-CV) in order to reduce bias. They also advocate for the selection of the simplest model that performs similarly to the best-scoring model, and they present a modified one-standard-error criteria to mitigate overfitting. With the help of two ecological case studies, the paper illustrates how to apply these methodologies and offers helpful advice. The paper concludes by highlighting the significance of predictive evaluation in model selection for a range of modeling objectives, such as hypothesis testing, exploration, and prediction, and by warning against forming biased conclusions after model selection.

## Note 3 [ @qi2019estimating ]

The paper "On Estimating Model in Feature Selection With Cross-Validation" investigates the effectiveness of different cross-validation (CV) methods for model evaluation during feature selection. The study examines five CV methods: leave-one-out (LOO), repeated 2-fold CV, repeated 5-fold CV, repeated 10-fold CV, and a nested 10-fold CV. It focuses on the hybrid feature selection algorithm FDHSFFS. The study tests the accuracy of error estimation, computational efficiency, and the choice of best models and feature subsets using four UCI datasets with different feature dimensions and sample subsets.

It includes the lowest errors produced with the least computational expense on low-dimensional datasets by 2-fold CV and LOO. Nested CV and 10-fold CV are computationally demanding yet yield more accurate error estimates on high-dimensional datasets. Though different CV techniques may use different optimal models, they frequently provide roughly the same optimal feature subsets. The nested 10-fold CV approach strikes a balance between computing cost and accuracy by achieving almost unbiased error estimation.






## References
