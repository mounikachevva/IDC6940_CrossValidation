---
title: "Prediction of Martensite Start Temperature in Steels Using Cross Validation"
author: "Mounika Chevva (Advisor: Dr. Samantha Seals)"
date: 'today'
execute:
  echo: true
  warning: false
  message: false
  error: false
  
format: 
  revealjs:
    theme: serif
    embed-resources: true
    slide-number: true
    width: 2000
    height: 1000
    df-print: paged
    scrollable: true
    html-math-method: katex
    bibliography: references.bib 
editor: source
pdf-separate-fragments: true
fig-align: center

self-contained: true

---

## Introduction 

#### Cross-validation Overview

- A statistical technique for evaluating the performance and generalizability of machine learning models.

- Divides dataset into training and validation subsets.

- Ensures model training on one subset and validation on another.

#### Advantages:

- Provides more reliable estimates of model performance.

- Reduces bias compared to a single train-test split.

- Improves model generalizability by leveraging different training and validation data.


## Methods

- K-Fold Cross-Validation: Dataset is split into $k$ folds, model is trained on $k-1$ folds and validated on the remaining fold, repeated $k$ times @Kohavi1995.

- Leave-One-Out Cross-Validation (LOOCV): A special case of K-Fold where $k$ equals the number of observations, each sample serves as the validation set once.

- Nested Cross-Validation: Used for model selection and hyperparameter tuning, an outer loop for validation and an inner loop for training and hyperparameter optimization.

## Model Measures of Error (MOE)

- Definition: Measures of Error (MOE) quantify the difference between predicted values and actual outcomes, helping assess model performance.

::: {.panel-tabset}

#### Mean Absolute Error (MAE):

- Measures the average absolute errors between predicted and actual values @willmott2005advantages..

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

where $y_i$ is the actual value, $\hat{y}_i$ is the predicted value,and $n$ is the total number of observations.

#### Root Mean Squared Error (RMSE):

- The square root of the MSE, providing error in the same units as the target variable @chai2014root.

$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

where $y_i$ is the observed value, $\hat{y}_i$ is the predicted value,and $n$ is the total number of observations.


####  R-squared (R²)

- Represents the proportion of variance in the dependent variable that can be explained by the independent variables @draper1998applied.

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$
where $\bar{y}$ is the mean of the actual values.

:::


## Cross Validation Methods

::: {.panel-tabset}

### K-Fold Cross-Validation 

1. **Divide the dataset** \(D\) into \(K\) equally sized subsets (folds).

2. **For each fold** \(k\) (where \(k = 1, 2, \ldots, K\)):
   - Train the model \(M\) on the \(K - 1\) folds and validate it on the \(k\)-th fold.
   - Calculate the performance metric \(P_k\) (e.g., accuracy, MAE) on the \(k\)-th fold.

3. **The overall performance metric** is then averaged over all \(K\) folds:

$$
\text{CV}(M) = \frac{1}{K} \sum_{k=1}^{K} P_k
$$

###  Leave-One-Out Cross-Validation (LOOCV)

1. **Divide the dataset** \(D\) into \(n\) subsets (where \(n\) is the number of observations).

2. **For each observation** \(i\) (where \(i = 1, 2, \ldots, n\)):
   - Train the model \(M\) on the remaining \(n - 1\) observations.
   - Validate the model on the \(i\)-th observation.
   - Calculate the performance metric \(P_i\) (e.g., accuracy, MAE) on the \(i\)-th observation.

3. **The overall performance metric** is then averaged over all \(n\) observations:

$$
\text{LOOCV}(M) = \frac{1}{n} \sum_{i=1}^{n} P_i
$$

### Nested Cross-Validation (Nested CV)

1. **Outer Loop**:
   - Divide the dataset \(D\) into \(K\) outer folds.
   - For each outer fold \(k\) (where \(k = 1, 2, \ldots, K\)):
     - Reserve the \(k\)-th fold as the validation set.
     - Use the remaining \(K - 1\) folds as the training set.

2. **Inner Loop**:
   - For each training set from the outer loop, perform \(M\) inner folds:
     - Divide the training set into \(M\) inner folds.
     - For each inner fold \(j\) (where \(j = 1, 2, \ldots, M\)):
       - Train the model \(M\) on the \(M - 1\) inner folds.
       - Validate it on the \(j\)-th inner fold.
       - Calculate the performance metric \(P_{kj}\) on the \(j\)-th inner fold.

3. **Performance Metrics**:
   - Average the inner loop performance metrics for each outer fold:
   
   $$
   \text{Inner CV}(M) = \frac{1}{M} \sum_{j=1}^{M} P_{kj}
   $$
   
   - The overall performance metric of the model is averaged over all outer folds:
   
   $$
   \text{Nested CV}(M) = \frac{1}{K} \sum_{k=1}^{K} \text{Inner CV}(M)
   $$

:::

## Data Exploration and Visualization

In our study, we analyzed a dataset from @wentzien2024machine Martensite dataset focuses on predicting the Martensite Start Temperature (Ms) in steel alloys based on their chemical compositions. 

- Martensite start temperature (Ms) is  target variable.
- "C","Mn","Si","Cr","Ni" are Predictor variables
```{r correlation-matrix, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Figure 2: Correlation_Matrix"}
library(ggplot2)
library(reshape2)
library(corrplot)
Martensite <- read.csv("C:/Users/LaptopUser/Documents/IDC6940_CrossValidation/Martensite Start-Temp.csv")
correlation_matrix <- cor(Martensite, use = "complete.obs")

#print(correlation_matrix)

corr_plot <- corrplot(correlation_matrix, method = "circle")


```


## Modeling and Results

### Linear Regression Model

Linear regression is a fundamental statistical technique that establishes a relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.

In our dataset, which focuses on predicting the Martensite Start Temperature (Ms) of steel based on its chemical composition (C, Mn, Si, Cr, Ni), linear regression allows us to quantify how changes in these elements influence Ms.


 $$ M_s = \beta_0 +\beta_1 C +\beta_2 Mn + \beta_3 Si + \beta_4 Cr + \beta_5 Ni $$

 
 $$M_s = 746.99 - 254.85 C - 24.24 Mn - 13.28 Si - 7.8 Cr - 14.64 Ni $$
 

***Statistics***

Residual standard error: 54.28 on 1230 degrees of freedom

Multiple R-squared:  0.7433,

Adjusted R-squared:  0.7422 

F-statistic: 712.2 on 5 and 1230 DF,  

p-value: < 2.2e-16


## Cross Validation Results for Linear Regression

The results of this analysis reveal that the models tested with 5-Fold Cross-Validation (5-Fold CV) and Leave-One-Out Cross-Validation (LOOCV) demonstrate impressive predictive accuracy. These two models consistently outshine the Nested CV model, indicating that they are more dependable for making predictions from the dataset.

![Linear Regression Cross-Validation](C:/Users/LaptopUser/Documents/IDC6940_CrossValidation/LR_CV.png)


## Support Vector Machines (SVM) for Regression (SVR)

Support Vector Machines (SVM) are algorithms that model data by finding optimal boundaries, handling nonlinear patterns using kernels.
Using our dataset, Support Vector Machines (SVM) with a radial kernel help predict the martensite start temperature (Ms) based on chemical elements like C, Mn, Ni, Si, and Cr. SVM works by finding the best way to capture the relationship between these variables, effectively handling complex patterns for accurate Ms predictions.

::: {.panel-tabset}

### SVM K-Fold Cross-Validation

![SVM K-Fold Cross-Validation](C:/Users/LaptopUser/Documents/IDC6940_CrossValidation/SVM_Kfold.png)


### SVM LOOCV Cross-Validation

![SVM LOOCV Cross-Validation](C:/Users/LaptopUser/Documents/IDC6940_CrossValidation/SVM_LOOCV.png)


### SVM Nested Cross-Validation

![SVM Nested Cross-Validation](C:/Users/LaptopUser/Documents/IDC6940_CrossValidation/SVM_Nested.png)
::: 

## Model Comparision Results

In this study, we compared the performance of Linear Regression and Support Vector Machine (SVM) models in predicting the martensite start temperature (Ms). Using 5-fold cross-validation, we found that SVM outperformed Linear Regression, achieving a lower MAE (~21 vs ~33), a higher R² (~0.9 vs ~0.55), and a lower RMSE (~25 vs ~48), highlighting its superior accuracy and reliability in making predictions.

![Model Comparison](C:/Users/LaptopUser/Documents/IDC6940_CrossValidation/ModelComparision.png)


## Model Comparision Plot

![Model Comparison Plot](C:/Users/LaptopUser/Documents/IDC6940_CrossValidation/ModelComparision_plot.png)

## Conclusion: Overview

### Evaluation of Two Models:

Linear Regression Model

Support Vector Machine Model

### Cross-validation Methods Used:

k-fold Cross-validation

Leave-one-out Cross-validation (LOOCV)

Nested Cross-validation

## Conclusion: Key Findings

- Mean Absolute Error (MAE): SVM performed much better, with a lower MAE (~21) compared to Linear Regression (~33), meaning SVM’s predictions were closer to the actual values.

- R-squared (R²): SVM showed a significantly higher R² (~0.9) than Linear Regression (~0.55), indicating that SVM explained 90% of the data’s variability, while Linear Regression only accounted for about 55%.

- Root Mean Squared Error (RMSE): SVM had a much lower RMSE (~25) compared to Linear Regression (~48), which reflects its greater accuracy and fewer large prediction errors.

- Overall Performance: Across all key metrics, SVM outperformed Linear Regression, proving to be a more accurate and reliable model for predicting martensite start temperature (Ms).



## References
